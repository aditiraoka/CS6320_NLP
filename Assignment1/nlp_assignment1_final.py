# -*- coding: utf-8 -*-
"""NLP_Assignment1_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yHPaaCLUejzsn3Zo_i5OulvfOvzcKxJS
"""

GITHUB_PRIVATE_KEY = """-----BEGIN OPENSSH PRIVATE KEY-----
b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW
QyNTUxOQAAACCwn4bEFn+BYhBEETvzCz6Z2CZ7fzlyWHtLBL6EqVy6RgAAAJhcn5o4XJ+a
OAAAAAtzc2gtZWQyNTUxOQAAACCwn4bEFn+BYhBEETvzCz6Z2CZ7fzlyWHtLBL6EqVy6Rg
AAAEChCmWiqUAZBvdNskSAg6+6xhW4qH/ymt4e6OLvtlc6v7CfhsQWf4FiEEQRO/MLPpnY
Jnt/OXJYe0sEvoSpXLpGAAAAEXJvb3RAZDVhMDlkOTI0ZjlkAQIDBA==
-----END OPENSSH PRIVATE KEY-----
"""

# Create the directory if it doesn't exist.
! mkdir -p /root/.ssh
# Write the key
with open("/root/.ssh/id_ed25519", "w") as f:
  f.write(GITHUB_PRIVATE_KEY)
# Add github.com to our known hosts
! ssh-keyscan -t ed25519 github.com >> ~/.ssh/known_hosts
# Restrict the key permissions, or else SSH will complain.
! chmod go-rwx /root/.ssh/id_ed25519

# Note the `git@github.com` syntax, which will fetch over SSH instead of
# HTTP.
! git clone git@github.com:aditiraoka/CS6320_NLP.git

# Import All needed Libraries
import math
import pandas as pd
import re
import string

## All The Functions

#remove punctuations . ? ! from the dataset
#add start and stop tags to each entry
def remove_punctuation(x):
    try:
      x = re.sub(r'[.?!,<>\(\):;\[\]]+', ' ', x)
      x=x.lower()
      x = str("<s> ")+x+str(" <st>")
    except:
        pass
    return x

#split data and calculate counts for unigram and bigram occurences
def ngram_counts(x, vocabulary, unigrams, bigrams):
  words = x.split(" ")

  for i in range(len(words)-1):
    if words[i] not in unigrams.keys():
      unigrams[words[i]] = 1
    else:
      unigrams[words[i]] += 1

    if (words[i], words[i+1]) not in bigrams.keys():
      bigrams[(words[i], words[i+1])] = 1
    else:
      bigrams[(words[i], words[i+1])] += 1

    if words[i] not in vocabulary:
      vocabulary.append(words[i])

  return vocabulary, unigrams, bigrams

# Counting Vocabulary
def val_vocabulary(x, val_vocab, count):
  words = x.split(" ")
  #count += len(words)
  for i in words:
    if i not in val_vocab:
      val_vocab.append(i)

  count = len(val_vocab)

  return val_vocab, count

#calculate unsmoothened unigram probabilities
def calc_probabilities(unigram_counts, bigram_counts):
  # Calculate unigram and bigram probabilities
    total_unigrams = sum(unigram_counts.values())

    unigram_probabilities = {}
    bigram_probabilities = {}

    for word, count in unigram_counts.items():
        unigram_probabilities[word] = count / total_unigrams

    for bigram, count in bigram_counts.items():
        prev_word, current_word = bigram
        bigram_probabilities[bigram] = count / unigram_counts[prev_word]

    return unigram_probabilities, bigram_probabilities

def add_k_smoothing(unigrams, bigrams, k):

  V_size = sum(unigrams.values())
  unique_words = len(unigrams.keys())

  #add_k smoothing for unigrams
  add_k_unigram = {}

  for word,count in unigrams.items():
    add_k_unigram[word] = (count + k) / (V_size + (k * unique_words))

  #add_k smoothing for bigrams
  add_k_bigram = {}

  for bigram, count in bigrams.items():
    prev, cur = bigram
    add_k_bigram[bigram] = (count + k) / (unigrams[prev] + (k * unique_words))


  return add_k_unigram, add_k_bigram

#smoothing unigram probabilities
def laplace_smoothing(unigrams, bigrams):
  v_size = sum(unigrams.values())
  unique_words = len(unigrams.keys())

  #laplace smoothing for unigrams
  laplace_unigram = {}

  for word,count in unigrams.items():
    laplace_unigram[word] = (count + 1) / (v_size + unique_words)

  #laplace smoothing for bigrams
  laplace_bigram = {}

  for bigram, count in bigrams.items():
    prev, cur = bigram
    laplace_bigram[bigram] = (count + 1) / (unigrams[prev] + unique_words)

  return laplace_unigram, laplace_bigram

#calculate perplexity for unigrams on validation set
def perplexity_unigram(val_vocab, N_test, unigram_prob):
  log_sum = 0
  log_prob = 0

  N = len(val_vocab)
  x = unigram_prob.get("<UNK>")

  for i in range(0, N):
    log_prob = (math.log(unigram_prob.get(val_vocab[i],x)))
    log_sum += log_prob

  l = -((1/N_test)*log_sum)
  unigram_perplexity = math.pow(2, l)

  return unigram_perplexity

#perplexity for bigrams for laplace and addk smoothing
def perplexity_bigram(val_vocab, N_test, bigram_prob):
  log_sum = 0
  log_prob = 0

  N = len(val_vocab)
  tup = ("<UNK>", "<UNK>")
  x = bigram_prob.get(tup)
  #print(x)

  for i in range(1,N):
    bigram = (val_vocab[i-1],val_vocab[i])
    log_prob = (math.log(bigram_prob.get(bigram,x)))
    log_sum += log_prob
  l = -((1/N_test)*log_sum)

  bigram_perplexity = math.pow(2, l)

  return bigram_perplexity

#Main Function

#Import the files
with open('/content/CS6320_NLP/Assignment1/data/train.txt') as f:
    train_file = f.read().splitlines()

train_df = pd.DataFrame(train_file)
sentences = train_df

## STEP 1
#Apply Pre-processing
sentences[0] = sentences[0].apply(remove_punctuation)

# Calculate Uni-gram, Bi-gram probabilities
vocabulary = []
unigrams = {}
bigrams = {}

# Word Count for Uni-gram, Bi-gram probabilities
for i in range(len(sentences)):
  vocabulary, unigrams, bigrams = ngram_counts(sentences[0][i], vocabulary, unigrams, bigrams)

#Calculating Probs
unigram_probabilities, bigram_probabilities = calc_probabilities(unigrams, bigrams)

#handling unknown words
unigrams["<UNK>"] = 0
bigrams[("<UNK>", "<UNK>")] = 0

# Calculate Smoothing
laplace_unigram, laplace_bigram = laplace_smoothing(unigrams, bigrams)
add_k_unigram, add_k_bigram = add_k_smoothing(unigrams,bigrams,k=0.45)
add_k_1_unigram, add_k_1_bigram = add_k_smoothing(unigrams,bigrams,k=0.5)

#print("laplace_unigram: "+str(laplace_unigram["<UNK>"]))
#print("laplace_bigram: "+str(laplace_bigram))

## Validation Part
with open('/content/CS6320_NLP/Assignment1/data/val.txt') as f:
    val_file = f.read().splitlines()

val_df = pd.DataFrame(val_file)
val_sentences = val_df
val_sentences[0] = val_sentences[0].apply(remove_punctuation)
#print(val_sentences)

val_vocab = []
N_test = 0
for i in range(len(val_sentences)):
  val_vocab, N_test = val_vocabulary(val_sentences[0][i], val_vocab, N_test)
#print("Val vocab count(N): "+ str(N_test))

#Perplexity for training dataset
N = len(vocabulary) - 1
laplace_uni = perplexity_unigram(vocabulary, N, laplace_unigram)
k_uni = perplexity_unigram(vocabulary, N, add_k_unigram)
k_1_uni = perplexity_unigram(vocabulary, N, add_k_1_unigram)

print("\nPerplexity on Training Set:")
print("Laplace Unigram Perplexity: "+str(laplace_uni))
print("Add-k unigram Perplexity(k=0.45): "+str(k_uni))
print("Add-k unigram Perplexity(k=0.5): "+str(k_1_uni))

#bigram
laplace_bi = perplexity_bigram(vocabulary, N, laplace_bigram)
k_bi = perplexity_bigram(vocabulary, N, add_k_bigram)
k_1_bi = perplexity_bigram(vocabulary, N, add_k_1_bigram)

print("\nLaplace Bigram Perplexity: "+str(laplace_bi))
print("Add-k Bigram Perplexity(k=0.45): "+str(k_bi))
print("Add-k Bigram Perplexity(k=0.5): "+str(k_1_bi))

#Perplexity for validation dataset
#Finding Uni-gram Perplexity
laplace_uni_pp = perplexity_unigram(val_vocab, N_test-1, laplace_unigram)
k_uni_pp = perplexity_unigram(val_vocab, N_test-1, add_k_unigram)
k_uni_1_pp = perplexity_unigram(val_vocab, N_test-1, add_k_1_unigram)

print("\nPerplexity on Validation Set:")
print("Laplace Unigram Perplexity: "+str(laplace_uni_pp))
print("Add-k unigram Perplexity(k=0.45): "+str(k_uni_pp))
print("Add-k unigram Perplexity(k=0.5): "+str(k_uni_1_pp))

# Calculating Bi-gram Perplexity
laplace_bi_pp = perplexity_bigram(val_vocab, N_test-1, laplace_bigram)
addk_bi_pp = perplexity_bigram(val_vocab, N_test-1, add_k_bigram)
addk_bi_1_pp = perplexity_bigram(val_vocab, N_test-1, add_k_1_bigram)

print("\nLaplace Bigram Perplexity: "+str(laplace_bi_pp))
print("Add-k Bigram Perplexity(k=0.45): "+str(addk_bi_pp))
print("Add-k Bigram Perplexity(k=0.5): "+str(addk_bi_1_pp))

