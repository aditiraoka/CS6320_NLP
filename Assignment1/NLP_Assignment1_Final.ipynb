{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwA3odjsS2Em",
        "outputId": "18b2f96a-a1d1-404b-bd23-8b4a709eb7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-babeld-dd067d10\n",
            "Cloning into 'CS6320_NLP'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (15/15), 431.71 KiB | 12.70 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "GITHUB_PRIVATE_KEY = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "QyNTUxOQAAACCwn4bEFn+BYhBEETvzCz6Z2CZ7fzlyWHtLBL6EqVy6RgAAAJhcn5o4XJ+a\n",
        "OAAAAAtzc2gtZWQyNTUxOQAAACCwn4bEFn+BYhBEETvzCz6Z2CZ7fzlyWHtLBL6EqVy6Rg\n",
        "AAAEChCmWiqUAZBvdNskSAg6+6xhW4qH/ymt4e6OLvtlc6v7CfhsQWf4FiEEQRO/MLPpnY\n",
        "Jnt/OXJYe0sEvoSpXLpGAAAAEXJvb3RAZDVhMDlkOTI0ZjlkAQIDBA==\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "\"\"\"\n",
        "\n",
        "# Create the directory if it doesn't exist.\n",
        "! mkdir -p /root/.ssh\n",
        "# Write the key\n",
        "with open(\"/root/.ssh/id_ed25519\", \"w\") as f:\n",
        "  f.write(GITHUB_PRIVATE_KEY)\n",
        "# Add github.com to our known hosts\n",
        "! ssh-keyscan -t ed25519 github.com >> ~/.ssh/known_hosts\n",
        "# Restrict the key permissions, or else SSH will complain.\n",
        "! chmod go-rwx /root/.ssh/id_ed25519\n",
        "\n",
        "# Note the `git@github.com` syntax, which will fetch over SSH instead of\n",
        "# HTTP.\n",
        "! git clone git@github.com:aditiraoka/CS6320_NLP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import All needed Libraries\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "xRwxV4jAqtAd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Xg2lZz3MT3Wt"
      },
      "outputs": [],
      "source": [
        "## All The Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuations . ? ! from the dataset\n",
        "#add start and stop tags to each entry\n",
        "def remove_punctuation(x):\n",
        "    try:\n",
        "      x = re.sub(r'[.?!,<>\\(\\):;\\[\\]]+', ' ', x)\n",
        "      x=x.lower()\n",
        "      x = str(\"<s> \")+x+str(\" <st>\")\n",
        "    except:\n",
        "        pass\n",
        "    return x"
      ],
      "metadata": {
        "id": "r2VJId72XupO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data and calculate counts for unigram and bigram occurences\n",
        "def ngram_counts(x, vocabulary, unigrams, bigrams):\n",
        "  words = x.split(\" \")\n",
        "\n",
        "  for i in range(len(words)-1):\n",
        "    if words[i] not in unigrams.keys():\n",
        "      unigrams[words[i]] = 1\n",
        "    else:\n",
        "      unigrams[words[i]] += 1\n",
        "\n",
        "    if (words[i], words[i+1]) not in bigrams.keys():\n",
        "      bigrams[(words[i], words[i+1])] = 1\n",
        "    else:\n",
        "      bigrams[(words[i], words[i+1])] += 1\n",
        "\n",
        "    if words[i] not in vocabulary:\n",
        "      vocabulary.append(words[i])\n",
        "\n",
        "  return vocabulary, unigrams, bigrams\n"
      ],
      "metadata": {
        "id": "iKfnyD3BNqJv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting Vocabulary\n",
        "def val_vocabulary(x, val_vocab, count):\n",
        "  words = x.split(\" \")\n",
        "  #count += len(words)\n",
        "  for i in words:\n",
        "    if i not in val_vocab:\n",
        "      val_vocab.append(i)\n",
        "\n",
        "  count = len(val_vocab)\n",
        "\n",
        "  return val_vocab, count"
      ],
      "metadata": {
        "id": "_5ecD4X-mIWu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate unsmoothened unigram probabilities\n",
        "def calc_probabilities(unigram_counts, bigram_counts):\n",
        "  # Calculate unigram and bigram probabilities\n",
        "    total_unigrams = sum(unigram_counts.values())\n",
        "\n",
        "    unigram_probabilities = {}\n",
        "    bigram_probabilities = {}\n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_probabilities[word] = count / total_unigrams\n",
        "\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        prev_word, current_word = bigram\n",
        "        bigram_probabilities[bigram] = count / unigram_counts[prev_word]\n",
        "\n",
        "    return unigram_probabilities, bigram_probabilities"
      ],
      "metadata": {
        "id": "FxlmopUAWbFA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_k_smoothing(unigrams, bigrams, k):\n",
        "\n",
        "  V_size = sum(unigrams.values())\n",
        "  unique_words = len(unigrams.keys())\n",
        "\n",
        "  #add_k smoothing for unigrams\n",
        "  add_k_unigram = {}\n",
        "\n",
        "  for word,count in unigrams.items():\n",
        "    add_k_unigram[word] = (count + k) / (V_size + (k * unique_words))\n",
        "\n",
        "  #add_k smoothing for bigrams\n",
        "  add_k_bigram = {}\n",
        "\n",
        "  for bigram, count in bigrams.items():\n",
        "    prev, cur = bigram\n",
        "    add_k_bigram[bigram] = (count + k) / (unigrams[prev] + (k * unique_words))\n",
        "\n",
        "\n",
        "  return add_k_unigram, add_k_bigram"
      ],
      "metadata": {
        "id": "g9qQFuiyMWD2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smoothing unigram probabilities\n",
        "def laplace_smoothing(unigrams, bigrams):\n",
        "  v_size = sum(unigrams.values())\n",
        "  unique_words = len(unigrams.keys())\n",
        "\n",
        "  #laplace smoothing for unigrams\n",
        "  laplace_unigram = {}\n",
        "\n",
        "  for word,count in unigrams.items():\n",
        "    laplace_unigram[word] = (count + 1) / (v_size + unique_words)\n",
        "\n",
        "  #laplace smoothing for bigrams\n",
        "  laplace_bigram = {}\n",
        "\n",
        "  for bigram, count in bigrams.items():\n",
        "    prev, cur = bigram\n",
        "    laplace_bigram[bigram] = (count + 1) / (unigrams[prev] + unique_words)\n",
        "\n",
        "  return laplace_unigram, laplace_bigram\n"
      ],
      "metadata": {
        "id": "HqFHDrH0kOhU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate perplexity for unigrams on validation set\n",
        "def perplexity_unigram(val_vocab, N_test, unigram_prob):\n",
        "  log_sum = 0\n",
        "  log_prob = 0\n",
        "\n",
        "  N = len(val_vocab)\n",
        "  x = unigram_prob.get(\"<UNK>\")\n",
        "\n",
        "  for i in range(0, N):\n",
        "    log_prob = (math.log(unigram_prob.get(val_vocab[i],x)))\n",
        "    log_sum += log_prob\n",
        "\n",
        "  l = -((1/N_test)*log_sum)\n",
        "  unigram_perplexity = math.pow(2, l)\n",
        "\n",
        "  return unigram_perplexity"
      ],
      "metadata": {
        "id": "ULhoNzVHO_pM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perplexity for bigrams for laplace and addk smoothing\n",
        "def perplexity_bigram(val_vocab, N_test, bigram_prob):\n",
        "  log_sum = 0\n",
        "  log_prob = 0\n",
        "\n",
        "  N = len(val_vocab)\n",
        "  tup = (\"<UNK>\", \"<UNK>\")\n",
        "  x = bigram_prob.get(tup)\n",
        "  #print(x)\n",
        "\n",
        "  for i in range(1,N):\n",
        "    bigram = (val_vocab[i-1],val_vocab[i])\n",
        "    log_prob = (math.log(bigram_prob.get(bigram,x)))\n",
        "    log_sum += log_prob\n",
        "  l = -((1/N_test)*log_sum)\n",
        "\n",
        "  bigram_perplexity = math.pow(2, l)\n",
        "\n",
        "  return bigram_perplexity"
      ],
      "metadata": {
        "id": "RXXbLhN7ufkl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Main Function"
      ],
      "metadata": {
        "id": "1x0BKHMRNR5a"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "OgUI6ogAOt9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb58ea48-cadd-440d-f069-13d3f1d21c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity on Training Set:\n",
            "Laplace Unigram Perplexity: 1181.0965618756832\n",
            "Add-k unigram Perplexity(k=0.45): 1341.9440697747366\n",
            "Add-k unigram Perplexity(k=0.5): 1323.9966185880676\n",
            "\n",
            "Laplace Bigram Perplexity: 396.4138882856394\n",
            "Add-k Bigram Perplexity(k=0.45): 379.77727806261004\n",
            "Add-k Bigram Perplexity(k=0.5): 382.1763576697586\n",
            "\n",
            "Perplexity on Validation Set:\n",
            "Laplace Unigram Perplexity: 693.545192441177\n",
            "Add-k unigram Perplexity(k=0.45): 783.029085353134\n",
            "Add-k unigram Perplexity(k=0.5): 771.2522912897416\n",
            "\n",
            "Laplace Bigram Perplexity: 396.17586734094004\n",
            "Add-k Bigram Perplexity(k=0.45): 385.8125310429353\n",
            "Add-k Bigram Perplexity(k=0.5): 387.22296936833163\n"
          ]
        }
      ],
      "source": [
        "#Import the files\n",
        "with open('/content/CS6320_NLP/Assignment1/data/train.txt') as f:\n",
        "    train_file = f.read().splitlines()\n",
        "\n",
        "train_df = pd.DataFrame(train_file)\n",
        "sentences = train_df\n",
        "\n",
        "## STEP 1\n",
        "#Apply Pre-processing\n",
        "sentences[0] = sentences[0].apply(remove_punctuation)\n",
        "\n",
        "# Calculate Uni-gram, Bi-gram probabilities\n",
        "vocabulary = []\n",
        "unigrams = {}\n",
        "bigrams = {}\n",
        "\n",
        "# Word Count for Uni-gram, Bi-gram probabilities\n",
        "for i in range(len(sentences)):\n",
        "  vocabulary, unigrams, bigrams = ngram_counts(sentences[0][i], vocabulary, unigrams, bigrams)\n",
        "\n",
        "#Calculating Probs\n",
        "unigram_probabilities, bigram_probabilities = calc_probabilities(unigrams, bigrams)\n",
        "\n",
        "#handling unknown words\n",
        "unigrams[\"<UNK>\"] = 0\n",
        "bigrams[(\"<UNK>\", \"<UNK>\")] = 0\n",
        "\n",
        "# Calculate Smoothing\n",
        "laplace_unigram, laplace_bigram = laplace_smoothing(unigrams, bigrams)\n",
        "add_k_unigram, add_k_bigram = add_k_smoothing(unigrams,bigrams,k=0.45)\n",
        "add_k_1_unigram, add_k_1_bigram = add_k_smoothing(unigrams,bigrams,k=0.5)\n",
        "\n",
        "#print(\"laplace_unigram: \"+str(laplace_unigram[\"<UNK>\"]))\n",
        "#print(\"laplace_bigram: \"+str(laplace_bigram))\n",
        "\n",
        "## Validation Part\n",
        "with open('/content/CS6320_NLP/Assignment1/data/val.txt') as f:\n",
        "    val_file = f.read().splitlines()\n",
        "\n",
        "val_df = pd.DataFrame(val_file)\n",
        "val_sentences = val_df\n",
        "val_sentences[0] = val_sentences[0].apply(remove_punctuation)\n",
        "#print(val_sentences)\n",
        "\n",
        "val_vocab = []\n",
        "N_test = 0\n",
        "for i in range(len(val_sentences)):\n",
        "  val_vocab, N_test = val_vocabulary(val_sentences[0][i], val_vocab, N_test)\n",
        "#print(\"Val vocab count(N): \"+ str(N_test))\n",
        "\n",
        "#Perplexity for training dataset\n",
        "N = len(vocabulary) - 1\n",
        "laplace_uni = perplexity_unigram(vocabulary, N, laplace_unigram)\n",
        "k_uni = perplexity_unigram(vocabulary, N, add_k_unigram)\n",
        "k_1_uni = perplexity_unigram(vocabulary, N, add_k_1_unigram)\n",
        "\n",
        "print(\"\\nPerplexity on Training Set:\")\n",
        "print(\"Laplace Unigram Perplexity: \"+str(laplace_uni))\n",
        "print(\"Add-k unigram Perplexity(k=0.45): \"+str(k_uni))\n",
        "print(\"Add-k unigram Perplexity(k=0.5): \"+str(k_1_uni))\n",
        "\n",
        "#bigram\n",
        "laplace_bi = perplexity_bigram(vocabulary, N, laplace_bigram)\n",
        "k_bi = perplexity_bigram(vocabulary, N, add_k_bigram)\n",
        "k_1_bi = perplexity_bigram(vocabulary, N, add_k_1_bigram)\n",
        "\n",
        "print(\"\\nLaplace Bigram Perplexity: \"+str(laplace_bi))\n",
        "print(\"Add-k Bigram Perplexity(k=0.45): \"+str(k_bi))\n",
        "print(\"Add-k Bigram Perplexity(k=0.5): \"+str(k_1_bi))\n",
        "\n",
        "#Perplexity for validation dataset\n",
        "#Finding Uni-gram Perplexity\n",
        "laplace_uni_pp = perplexity_unigram(val_vocab, N_test-1, laplace_unigram)\n",
        "k_uni_pp = perplexity_unigram(val_vocab, N_test-1, add_k_unigram)\n",
        "k_uni_1_pp = perplexity_unigram(val_vocab, N_test-1, add_k_1_unigram)\n",
        "\n",
        "print(\"\\nPerplexity on Validation Set:\")\n",
        "print(\"Laplace Unigram Perplexity: \"+str(laplace_uni_pp))\n",
        "print(\"Add-k unigram Perplexity(k=0.45): \"+str(k_uni_pp))\n",
        "print(\"Add-k unigram Perplexity(k=0.5): \"+str(k_uni_1_pp))\n",
        "\n",
        "# Calculating Bi-gram Perplexity\n",
        "laplace_bi_pp = perplexity_bigram(val_vocab, N_test-1, laplace_bigram)\n",
        "addk_bi_pp = perplexity_bigram(val_vocab, N_test-1, add_k_bigram)\n",
        "addk_bi_1_pp = perplexity_bigram(val_vocab, N_test-1, add_k_1_bigram)\n",
        "\n",
        "print(\"\\nLaplace Bigram Perplexity: \"+str(laplace_bi_pp))\n",
        "print(\"Add-k Bigram Perplexity(k=0.45): \"+str(addk_bi_pp))\n",
        "print(\"Add-k Bigram Perplexity(k=0.5): \"+str(addk_bi_1_pp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JR_uxp6ZhQWN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}